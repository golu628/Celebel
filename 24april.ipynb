{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtMXLtes+uEHuUYECsvBaw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/Celebel/blob/main/24april.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_6ZJI1UjcBG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Projections and PCA:\n",
        "\n",
        "A projection, in linear algebra, refers to taking a higher dimensional data point and mapping it onto a lower dimensional subspace. Imagine a shadow â€“ it's a 2D projection of a 3D object.\n",
        "In PCA, projections are used to create new features (principal components) that capture the most variance in the data. The data points are projected onto these principal components, effectively reducing dimensionality.\n",
        "Q2. Optimization in PCA:\n",
        "\n",
        "PCA aims to find a set of uncorrelated directions (principal components) that explain the maximum variance in the data. It's essentially an optimization problem.\n",
        "Mathematically, PCA maximizes the variance captured by the projected data points on each principal component, one at a time.\n",
        "Q3. Covariance Matrices and PCA:\n",
        "\n",
        "The covariance matrix captures the linear relationships between features in your data. It shows how much two features tend to vary together.\n",
        "PCA utilizes the covariance matrix to identify the directions of maximum variance. Eigenvectors (directions) and eigenvalues (amount of variance) of the covariance matrix are key outputs used to determine the principal components.\n",
        "Q4. Number of Principal Components:\n",
        "\n",
        "The number of principal components you choose impacts the trade-off between capturing variance and dimensionality reduction.\n",
        "Selecting more components retains more information but reduces dimensionality less. Conversely, fewer components lead to more aggressive dimensionality reduction but might miss important information.\n",
        "Q5. PCA for Feature Selection:\n",
        "\n",
        "PCA can be used for feature selection by identifying the principal components with the highest variance. These components likely contain the most relevant information for your analysis.\n",
        "Benefits include:\n",
        "Reduced model complexity by eliminating redundant features.\n",
        "Improved model performance by focusing on informative features.\n",
        "Reduced risk of overfitting.\n",
        "Q6. Applications of PCA:\n",
        "\n",
        "PCA is widely used in data science and machine learning for:\n",
        "Dimensionality reduction: preparing data for further analysis by reducing features without significant information loss.\n",
        "Visualization: helping visualize high-dimensional data by projecting it onto lower dimensions for easier interpretation.\n",
        "Anomaly detection: identifying data points that deviate significantly from the principal components, potentially indicating anomalies.\n",
        "Feature engineering: creating new features (principal components) that might be more informative than the original ones.\n",
        "Q7. Spread and Variance in PCA:\n",
        "\n",
        "Spread (or dispersion) and variance are related concepts. Variance is the average squared deviation from the mean, essentially measuring how spread out the data is.\n",
        "PCA leverages variance to identify principal components. Components corresponding to higher variance directions capture the data with the most spread, which is likely the most informative part.\n",
        "Q8. Identifying Principal Components:\n",
        "\n",
        "PCA uses the variance of the data projected onto different directions to identify principal components.\n",
        "The components aligned with the directions of highest variance capture the most spread in the data and are chosen as the principal components.\n",
        "Q9. PCA and Unequal Variance:\n",
        "\n",
        "PCA inherently prioritizes features with higher variance during the optimization process. This means components corresponding to high-variance dimensions will have a stronger influence on the principal components compared to low-variance ones.\n",
        "Here are some things to consider:\n",
        "Normalization: Standardizing the data before applying PCA ensures all features are on the same scale, preventing features with naturally high scales from dominating the principal components.\n",
        "Choosing the Right Number of Components: If capturing information from low-variance dimensions is crucial, you might need to retain more principal components even if it means less dimensionality reduction."
      ],
      "metadata": {
        "id": "hDGtCnQOj4SV"
      }
    }
  ]
}